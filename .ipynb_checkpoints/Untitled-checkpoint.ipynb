{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18a31841-1cbf-48ec-bf55-723333890b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from Patterns import *\n",
    "from tld import get_tld\n",
    "import ssl, socket\n",
    "from datetime import datetime as dt, timezone\n",
    "from dateutil.parser import parse\n",
    "import whois\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import warnings\n",
    "\n",
    "def Ip_Address(url, req, tld, domain_info):\n",
    "    ip = re.findall(r'[0-9]{1,3}(\\.[0-9]{1,3}){3}', url)\n",
    "    if ip:\n",
    "        return -1\n",
    "    else:\n",
    "        return 1\n",
    "def long_Address(url, req, tld, domain_info):\n",
    "    long = len(url)\n",
    "    if long < 54:\n",
    "        return 1\n",
    "    if long <= 75:\n",
    "        return 0\n",
    "    return -1\n",
    "def Shortening_Service(url,req,tld,domain_info):\n",
    "    match = re.search(short_url, url)\n",
    "    if match :\n",
    "        return -1\n",
    "    else :\n",
    "        return 1\n",
    "def at_symbol(url, req, tld, domain_info):\n",
    "    if '@' in url or '%40' in url:\n",
    "        return -1\n",
    "    return 1\n",
    "def double_slash(url, req, tld, domain_info):\n",
    "    if '//' in url[7:]:\n",
    "        return -1\n",
    "    return 1\n",
    "def dash_symbol(url, req, tld, domain_info):\n",
    "    if '-' in tld.domain or '-' in tld.subdomain:\n",
    "        return -1\n",
    "    return 1\n",
    "def having_subDomain(url, req, tld, domain_info):\n",
    "    url = url.replace('www.', '')\n",
    "    dot = 0\n",
    "    dot += str(tld).count('.')\n",
    "    if tld.subdomain:\n",
    "        dot += 1\n",
    "        dot += tld.subdomain.count('.')\n",
    "    if dot <= 1:\n",
    "        return 1\n",
    "    if dot == 2:\n",
    "        return 0\n",
    "    return -1\n",
    "def https_connect(url, req, tld, domain_info):\n",
    "    if \"https://\" in url:\n",
    "        url = url.replace(\"https://\", \"\")\n",
    "    elif \"http://\" in url:\n",
    "        url = url.replace(\"http://\", \"\")\n",
    "    ctx = ssl.create_default_context()\n",
    "    s = ctx.wrap_socket(socket.socket(), server_hostname=url)\n",
    "    s.connect((url, 443))\n",
    "    return s\n",
    "def SSLfinal_State(url, req, tld, domain_info):\n",
    "    try:\n",
    "        s = https_connect(url, req, rld, domain_info)\n",
    "    except:\n",
    "        return -1\n",
    "    cert = s.getpeercert()\n",
    "    notBefore = cert[\"notBefore\"]\n",
    "    try:\n",
    "        init_date = parse(notBefore)\n",
    "        now = dt.now()\n",
    "        total_days = (now.date() - init_date.date()).days\n",
    "    except:\n",
    "        return -1\n",
    "    if total_days >= 365:\n",
    "        return 1\n",
    "    return 0\n",
    "def Domain_registration_length(url, req, tld, domain_info):\n",
    "    try:\n",
    "        expirty_time = domain_info[\"expiration_date\"]\n",
    "    except:\n",
    "        return -1\n",
    "    now = dt.now().date()\n",
    "    if isinstance(expirty_time, list):\n",
    "        for expirty in expirty_time:\n",
    "            end_days = (expirty.date() - now).days\n",
    "            if end_days > 365:\n",
    "                return 1\n",
    "    else:\n",
    "        try:\n",
    "            end_days = (expirty_time.date() - now).days\n",
    "        except: \n",
    "            return -1\n",
    "        if end_days > 365:\n",
    "            return 1\n",
    "    return -1\n",
    "def Favicon(url, req, tld, domain_info):\n",
    "    try:\n",
    "        html = req.text\n",
    "    except:\n",
    "        return -1\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    tag_link = soup.find(\"link\", rel=re.compile(\"^icon$\", re.I))\n",
    "\n",
    "    if not tag_link:\n",
    "        return 0\n",
    "\n",
    "    domain = tld.domain\n",
    "    try:\n",
    "        if 'http' in tag_link[\"href\"]:\n",
    "            if domain in tag_link[\"href\"]:\n",
    "                return 1\n",
    "            else:\n",
    "                return -1\n",
    "    except:\n",
    "        return -1\n",
    "    return 1\n",
    "def HTTPS_token(url, req, tld, domain_info):\n",
    "    domain = tld.domain\n",
    "    subDomain = tld.subdomain\n",
    "    if \"https\" in domain or \"https\" in subDomain:\n",
    "        return -1\n",
    "    return 1\n",
    "def Request_URL(url, req, tld, domain_info):\n",
    "    try:\n",
    "        html = req.text\n",
    "    except:\n",
    "        return -1\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    domain = tld.domain\n",
    "\n",
    "    aTags = soup.findAll('a', href=True)\n",
    "    linkTags = soup.findAll('link', href=True)\n",
    "    imgTags = soup.findAll('img', src=True)\n",
    "    scriptTags = soup.findAll('script', src=True)\n",
    "    iframeTags = soup.findAll('iframe', src=True)\n",
    "\n",
    "    total = len(aTags) + len(linkTags) + len(imgTags) + len(scriptTags) + len(iframeTags)\n",
    "\n",
    "    count = 0\n",
    "    try:\n",
    "        for a in aTags:\n",
    "            if 'http' in a[\"href\"]:\n",
    "                if domain not in a[\"href\"]:\n",
    "                    count += 1\n",
    "        for link in linkTags:\n",
    "            if 'http' in link[\"href\"]:\n",
    "                if domain not in link[\"href\"]:\n",
    "                    count += 1\n",
    "        for img in imgTags:\n",
    "            if 'http' in img[\"src\"]:\n",
    "                if domain not in img[\"src\"]:\n",
    "                    count += 1\n",
    "        for script in scriptTags:\n",
    "            if 'http' in script[\"src\"]:\n",
    "                if domain not in script[\"src\"]:\n",
    "                    count += 1\n",
    "        for iframe in iframeTags:\n",
    "         if 'http' in iframe[\"src\"]:\n",
    "                if domain not in iframe[\"src\"]:\n",
    "                    count += 1\n",
    "    except:\n",
    "        return -1\n",
    "    try:\n",
    "        score = count / total * 100\n",
    "    except:\n",
    "        return -1\n",
    "\n",
    "    if score < 22:\n",
    "        return 1\n",
    "    elif 22 <= score < 61:\n",
    "        return 0\n",
    "    else:\n",
    "        return -1\n",
    "def URL_of_Anchor(url, req, tld, domain_info):\n",
    "    try:\n",
    "        html = req.text\n",
    "    except:\n",
    "        return -1\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    aTags = soup.findAll('a', href=True)\n",
    "\n",
    "    domain = tld.domain\n",
    "\n",
    "    count = 0\n",
    "    cstr = ['#', '#content', '#skip', 'javascript::void(0)']\n",
    "    try:\n",
    "        for a in aTags:\n",
    "            href = a[\"href\"]\n",
    "            if 'http' in href:\n",
    "                if domain not in href:\n",
    "                    count += 1\n",
    "            else:\n",
    "                for c in cstr:\n",
    "                    if c == href:\n",
    "                        count += 1\n",
    "    except:\n",
    "        return -1\n",
    "    try:\n",
    "        score = count / len(aTags) * 100\n",
    "    except:\n",
    "            return -1\n",
    "    if score < 31:\n",
    "        return 1\n",
    "    elif 31 <= score < 67:\n",
    "        return 0\n",
    "    else:\n",
    "        return -1\n",
    "def Links_in_tags(url, req, tld, domain_info):\n",
    "    try:\n",
    "        html = req.text\n",
    "    except:\n",
    "        return -1\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    domain = tld.domain\n",
    "\n",
    "    linkTags = soup.findAll('link', href=True)\n",
    "    scriptTags = soup.findAll('script', src=True)\n",
    "    metaTags = soup.findAll('meta', content=True)\n",
    "\n",
    "    total = len(linkTags) + len(scriptTags) + len(metaTags)\n",
    "\n",
    "    count = 0\n",
    "    try:\n",
    "        for link in linkTags:\n",
    "            if 'http' in link[\"href\"]:\n",
    "                if domain not in link[\"href\"]:\n",
    "                    count += 1\n",
    "        for script in scriptTags:\n",
    "            if 'http' in script[\"src\"]:\n",
    "                if domain not in script[\"src\"]:\n",
    "                    count += 1\n",
    "        for meta in metaTags:\n",
    "            if 'http' in meta[\"content\"]:\n",
    "                if domain not in meta[\"content\"]:\n",
    "                    count += 1\n",
    "    except:\n",
    "        return -1\n",
    "\n",
    "    try:\n",
    "        score = count / total * 100\n",
    "    except:\n",
    "        return -1\n",
    "    if score < 17:\n",
    "        return 1\n",
    "    elif 17 <= score < 81:\n",
    "        return 0\n",
    "    else:\n",
    "        return -1\n",
    "def SFH(url, req, tld, domain_info):\n",
    "    try:\n",
    "        html = req.text\n",
    "    except:\n",
    "        return -1\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    domain = tld.domain\n",
    "\n",
    "    formTags = soup.findAll('form', action=True)\n",
    "    if not formTags:\n",
    "        return 0\n",
    "\n",
    "    form = formTags[0]\n",
    "    if form[\"action\"].replace(' ','')  == \"about:blank\":\n",
    "        return -1\n",
    "    if 'http' in form[\"action\"]:\n",
    "        if domain not in form[\"action\"]:\n",
    "            return 0\n",
    "    return 1\n",
    "def Abnormal_URL(url, req, tld, domain_info):\n",
    "    domain = tld.domain\n",
    "\n",
    "    try:\n",
    "        if domain.lower() in domain_info['org'].lower():\n",
    "            return 1\n",
    "        return -1\n",
    "    except:\n",
    "        return 0\n",
    "def Age_of_Domain(url, req, tld, domain_info):\n",
    "    try:\n",
    "        create_time = domain_info[\"creation_date\"]\n",
    "    except:\n",
    "        return -1\n",
    "\n",
    "    now = dt.now().date()\n",
    "    if isinstance(create_time, list):\n",
    "        for create in create_time:\n",
    "            try:\n",
    "                age_days = (now - create.date()).days\n",
    "                if age_days >= 180:\n",
    "                    return 1\n",
    "            except:\n",
    "                return -1\n",
    "    else:\n",
    "        try:\n",
    "            age_days = (now - create_time.date()).days\n",
    "            if age_days >= 180:\n",
    "                return 1\n",
    "        except:\n",
    "            return -1\n",
    "    return -1\n",
    "\n",
    "def total_feature(url):\n",
    "    data_list = []\n",
    "    try:\n",
    "        req = requests.get(url, verify=False)\n",
    "    except:\n",
    "        try:\n",
    "            req = requests.get(\"http://\"+url, verify=False)\n",
    "        except:\n",
    "            req = -1\n",
    "    \n",
    "    try:\n",
    "        tld = get_tld(url, as_object=True, fix_protocol=True)\n",
    "    except:\n",
    "        tld = -1\n",
    "    try:\n",
    "        domain_info = whois.whois(url)\n",
    "    except:\n",
    "        domain_info = -1\n",
    "        \n",
    "        data_list.append([Ip_Address(url, req, tld, domain_info) ,long_Address(url, req, tld, domain_info) ,Shortening_Service(url, req, tld, domain_info) ,at_symbol(url, req, tld, domain_info) ,double_slash(url, req, tld, domain_info) ,dash_symbol(url, req, tld, domain_info) ,having_subDomain(url, req, tld, domain_info)  ,SSLfinal_State(url, req, tld, domain_info) ,-1 ,Favicon(url, req, tld, domain_info) ,HTTPS_token(url, req, tld, domain_info) ,Request_URL(url, req, tld, domain_info) ,URL_of_Anchor(url, req, tld, domain_info) ,Links_in_tags(url, req, tld, domain_info) ,SFH(url, req, tld, domain_info) ,-1 ,-1 ,-1])\n",
    "        return data_list\n",
    "    \n",
    "    data_list.append([Ip_Address(url, req, tld, domain_info) ,long_Address(url, req, tld, domain_info) ,Shortening_Service(url, req, tld, domain_info) ,at_symbol(url, req, tld, domain_info) ,double_slash(url, req, tld, domain_info) ,dash_symbol(url, req, tld, domain_info) ,having_subDomain(url, req, tld, domain_info)  ,SSLfinal_State(url, req, tld, domain_info) ,Domain_registration_length(url, req, tld, domain_info) ,Favicon(url, req, tld, domain_info) ,HTTPS_token(url, req, tld, domain_info) ,Request_URL(url, req, tld, domain_info) ,URL_of_Anchor(url, req, tld, domain_info) ,Links_in_tags(url, req, tld, domain_info) ,SFH(url, req, tld, domain_info) ,Abnormal_URL(url, req, tld, domain_info) ,Age_of_Domain(url, req, tld, domain_info) ,1])\n",
    "    return data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "74b5dee9-e54f-4142-888b-04226b81cc4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, -1, 1, 1, 1, 1, 1, -1, -1, -1, 1, -1, -1, -1, -1, -1, -1, -1]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_feature('https://storageapi.fleek.co/64b61f2d-d14f-44e5-b36d-85f878758f5a-bucket/index.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef028c8-1738-4042-a96c-0aa8f8c45c78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
